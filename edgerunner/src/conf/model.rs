use super::which::Which;
use serde::{Deserialize, Serialize};
use tokenizers::Tokenizer;

#[derive(Serialize, PartialEq, Deserialize, Debug)]
pub struct InferenceConfig {
    // GGML file to load, typically a .bin file generated by the quantize command from llama.cpp
    pub model: Option<String>,
    /// The temperature used to generate samples.
    pub temperature: Option<f64>,
    /// Nucleus sampling probability cutoff.
    pub top_p: Option<f64>,
    /// The seed to use when generating random samples.
    pub seed: u64,
    /// The length of the sample to generate (in tokens).
    pub sample_len: usize,
    /// Penalty to be applied for repeating tokens, 1. means no penalty.
    pub repeat_penalty: f32,
    /// The context size to consider for the repeat penalty.
    pub repeat_last_n: usize,
    /// The tokenizer config in json format.
    pub tokenizer: Option<String>,
    /// Display the token for the specified prompt.
    pub verbose_prompt: bool,
    /// The model size to use.
    pub which: Which,
}

impl Default for InferenceConfig {
    fn default() -> Self {
        InferenceConfig {
            model: None, // or Some("default_model_path".to_string()) if there is a default model
            temperature: Some(0.8),
            top_p: Some(0.9),
            seed: 299792458,
            sample_len: 1000,
            repeat_penalty: 1.1,
            repeat_last_n: 64,
            tokenizer: None,
            verbose_prompt: true,
            which: Which::Mistral7bInstruct,
        }
    }
}

impl InferenceConfig {
    pub fn tokenizer(&self) -> anyhow::Result<Tokenizer> {
        let tokenizer_path = match &self.tokenizer {
            Some(config) => std::path::PathBuf::from(config),
            None => {
                let api = hf_hub::api::sync::Api::new()?;
                let repo = self.which.tokenizer_repo();
                let api = api.model(repo.to_string());
                api.get("tokenizer.json")?
            }
        };
        Tokenizer::from_file(tokenizer_path).map_err(anyhow::Error::msg)
    }

    pub fn model(&self) -> anyhow::Result<std::path::PathBuf> {
        let model_path = match &self.model {
            Some(config) => std::path::PathBuf::from(config),
            None => {
                let (repo, filename) = self.which.get_repo_and_filename();

                let api = hf_hub::api::sync::Api::new()?;
                let api = api.model(repo.to_string());
                api.get(filename)?
            }
        };
        Ok(model_path)
    }
}
